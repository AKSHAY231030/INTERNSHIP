{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bd7b71",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8980ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "        return None\n",
    "def extract_header_tags(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    header_texts = [header.text.strip() for header in headers]\n",
    "    return header_texts\n",
    "def main():\n",
    "    html_content = get_html_content(wikipedia_url)\n",
    "    \n",
    "    if html_content:\n",
    "        headers = extract_header_tags(html_content)\n",
    "        \n",
    "        df = pd.DataFrame({'Headers': headers})\n",
    "        \n",
    "        print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783d9f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found on the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "presidents_url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "response = requests.get(presidents_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "if table:\n",
    "    data = []\n",
    "    \n",
    "  \n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    \n",
    "   \n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        row_data = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
    "        data.append(row_data)\n",
    "    \n",
    "   \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "   \n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Table not found on the webpage.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB SCRAPPING\n",
    "\n",
    "# Answer 1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "        return None\n",
    "def extract_header_tags(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    header_texts = [header.text.strip() for header in headers]\n",
    "    return header_texts\n",
    "def main():\n",
    "    html_content = get_html_content(wikipedia_url)\n",
    "    \n",
    "    if html_content:\n",
    "        headers = extract_header_tags(html_content)\n",
    "        \n",
    "        df = pd.DataFrame({'Headers': headers})\n",
    "        \n",
    "        print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# Answer 3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_rankings(url, table_class):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    data = []\n",
    "    table = soup.find('table', class_=table_class)\n",
    "\n",
    "    if table:\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "        data = [[cell.text.strip() for cell in row.find_all(['th', 'td'])] for row in table.find_all('tr')[1:]]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df.head(10)\n",
    "\n",
    "def main():\n",
    "\n",
    "    odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "  \n",
    "    odi_teams_df = scrape_rankings(odi_teams_url, 'table rankings-table')\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    print(odi_teams_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    odi_batsmen_df = scrape_rankings(odi_batsmen_url, 'table rankings-table')\n",
    "    print(\"Top 10 ODI Batsmen:\")\n",
    "    print(odi_batsmen_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    odi_bowlers_df = scrape_rankings(odi_bowlers_url, 'table rankings-table')\n",
    "    print(\"Top 10 ODI Bowlers:\")\n",
    "    print(odi_bowlers_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# Answer 2 \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "presidents_url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "response = requests.get(presidents_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "if table:\n",
    "    data = []\n",
    "    \n",
    "  \n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    \n",
    "   \n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        row_data = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
    "        data.append(row_data)\n",
    "    \n",
    "   \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "   \n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Table not found on the webpage.\")\n",
    "    \n",
    "\n",
    "\n",
    "# Answer 4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details():\n",
    "    # URL of the page containing restaurant details\n",
    "    dineout_url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(dineout_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extracting data for Restaurant name, Cuisine, Location, Ratings, and Image URL\n",
    "        restaurant_data = []\n",
    "        for restaurant in soup.find_all('div', class_='restnt-card'):\n",
    "            name = restaurant.find('div', class_='restnt-info cursor-pointer').h2.text.strip()\n",
    "            cuisine = restaurant.find('span', class_='double-line-ellipsis').text.strip()\n",
    "            location = restaurant.find('div', class_='restnt-loc cursor-pointer').text.strip()\n",
    "            ratings = restaurant.find('span', class_='double-line-ellipsis rating-text').text.strip()\n",
    "            image_url = restaurant.find('div', class_='image-section').img['src']\n",
    "\n",
    "            restaurant_data.append([name, cuisine, location, ratings, image_url])\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(restaurant_data, columns=['Restaurant Name', 'Cuisine', 'Location', 'Ratings', 'Image URL'])\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {dineout_url}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Scrape and display restaurant details\n",
    "    dineout_df = scrape_dineout_details()\n",
    "\n",
    "    if dineout_df is not None:\n",
    "        print(\"Restaurant Details:\")\n",
    "        print(dineout_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0d80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467eddb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3256a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ef5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec3eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1738b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996975f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce60f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0a820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fef55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee64e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2837d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c990be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1645e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78840fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3c43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af190266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea3cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7ced3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793dc095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5067b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296638a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4b37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c8283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fde9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a96982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6234cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb2dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29af27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3e7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327621a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
